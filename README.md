Sequential Kolmogorov-Smirnov tests with power 1
================================================

This is an implementation of Darling and Robbins's Kolmogorov-Smirnov
test described in [nonparametric sequential tests with power one](https://www.pnas.org/content/pnas/61/3/804.full.pdf).

The interface and usage is similar to the [Confidence Sequence
Method](https://github.com/pkhuong/csm).  However, rather than testing
a binomial distribution for its parameter, this library offers a
sequential versions of the
[Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)
test for goodness of fit between arbitrary distributions over ordered
values.

Like the [CSM](https://arxiv.org/abs/1611.01675), the methods can
safely be ran an unbounded number of times as the sample gains more
data points: the false positive (rejection of the null hypothesis when
the two distributions are actually equal) for an unbounded number of
trials is bounded by the `log_eps` parameters, since the method
internally corrects for multiple hypothesis testing.  Each new
observation increases the stringency of the test, so any actual
difference will eventually (with high probability) be detected, if we
run the method for long enough.

*If we let the method run forever*, we thus have a semialgorithm with
one-sided error: while we may spuriously reject the null hypothesis
with probability controlled by the `log_eps` parameter, we will always
eventually correctly reject the null.

The basic one-tailed test
-------------------------

The code directly implements the one-tailed test with two samples.
Given an unbounded stream of observations from distribution A and
distribution B, this one-tailed test determines whether the cumulative
distribution function (CDF) for A is always less than or equal to that
for B.  When the values track response latencies, this is equivalent
to asking whether, for every quantile, A is always slower than B.  If
the test rejects the null hypothesis, this means there is at least one
quantile at which A is faster than B.

There are two knobs over the expected runtime to correctly reject the
null hypothesis: the rate of false positive, and the number of
observations we accumulate before attempting to pass judgement.

Allowing a higher probability of falsely rejecting the null lets us be
more aggressive.  In the extreme, allowing 100% false positive would
let us reject the null before observing a single value (: We can
usually afford large sample sizes when data are generated by running
computer programs, so we should aim for very low false positive rates.
That's why the interface accepts the *natural log of* the allowed
false positive rate, `log_eps`.

The `min_count` is the minimum number of observations we'll accumulate
before testing the hypothesis. With more data, we can be more
confident, so not testing the hypothesis at all when we have too
little data lets us spend our "false positive budget" later, and thus
also be slightly more aggressive.

Given `n` pairs of data point, `one_sided_ks_threshold(n, min_count,
log_eps)` will give us the threshold in maximum CDF difference to
reject the null this iteration.  We can then look at the two empirical
CDFs, and find the value for with the CDF of A is maximally greater
than that of B.  If there is none (the empirical CDF of A is always
lower than that of B), we can't reject the null hypothesis.  Whenever
the difference is greater than the value returned by
`one_sided_ks_threshold`, we can reject the null hypothesis.
Otherwise, we must wait and get more data.

Additional tests
----------------

We sometimes want a two-tailed test, where we simply want to know if
two samples come from different underlying distributions.  That's
equivalent to refuting that the two CDFs are equivalent.  Instead of
comparing the maximum difference between the two CDF against
`one_sided_ks_threshold`, we can simply compare the maximum absolute
difference between the two CDFs against `one_sided_ks_threshold(n,
min_count, log_eps + one_sided_ks_pair_eq)`.  Adding that constant is
equivalent to dividing the allowed false positive rate by 2, i.e.,
we're simultaneously testing whether `CDF A >= CDF B` and vice versa,
with a [Bonferroni
correction](https://en.wikipedia.org/wiki/Bonferroni_correction).

We need a similar correction when testing to refute that an empirical
CDF is less than or equal to a fixed distribution: add
`one_sided_ks_fixed_le` to the original `log_eps`.  When testing for
equality with a fixed distribution, add `one_sided_ks_fixed_eq`.

Finally, we can also test whether points from a single sample come
from any distribution in a family of distributions (e.g., the set of
all univariate Normal distributions).  See [Darling and
Robbins](https://www.pnas.org/content/pnas/61/3/804.full.pdf) for more
details.  In that case, we'll find the instantiation that minimises
the difference between the empirical CDF and the distribution, and
compare that difference with the value returned by
`one_sided_ks_threshold`, with `log_eps = log(false_positive) + one_sided_ks_class`.

Discontinuous distributions
---------------------------

The classic Kolmogorov-Smirnov (e.g., as described by Darling and
Robbins) is defined on continuous distributions, without
discontinuities (e.g., not jump in CDF between 0.99999... and 1).  In
practice, discontinuous distributions are useful and show up a lot.

Thankfully, treating the distance between two discrete (or mixed)
distributions as though they were continuous does not compromise
correctness.  It simply means that we're more likely to accept the
null hypothesis. See [this 2017 paper](http://openaccess.city.ac.uk/18541/)
for more details, examples, and an idea of what the adjustment might
look like.  Our tests all have power 1, so this really means that
ignoring the discontinuity (and associated lower degrees of freedom)
simply means that we'll require more data to reject the null
hypothesis, when it does not actually hold.

More notes on usage
-------------------

If you're not sure whether your compiler is standard compliant, check
that `one_sided_ks_check_constants` returns 0.

Not all pairs of `log_eps` and `min_count` are valid.  Check validity
with `one_sided_ks_min_count_valid`.  When `min_count` is invalid,
that's because it is too low.  `one_sided_ks_find_min_count` will find
the least valid `min_count` value for a given `log_eps`.

Finally, one might want a terminating algorithm rather than a
semialgorithm that also has power one.  For such practically minded
people, there is `one_sided_ks_expected_iter`.  Given a (valid) pair
of parameters `min_count` and `log_eps`, and `delta`, the actual
maximum distance between two underlying CDFs,
`one_sided_ks_expected_iter` will return an upper bound on the
expected number of iterations needed to correctly reject the null
hypothesis.  That upper bound tends to be fairly conservative, but is
nevertheless valid.

We can also convert it into even more conservative bounds on the
median (or other quantile) number of iterations since we know that the
distribution of iteration counts is long-tailed and never goes below
`min_count`.  Simply knowing that the iteration count is a
non-negative value lets us apply
[Markov's inequality](https://en.wikipedia.org/wiki/Markov%27s_inequality):
the 90th percentile can't exceed 10x the expected iteration count.
We can do even better if we also take into account the fact that the
iteration count until we reject the null is always at least
`min_count`, and consider `iteration_count - min_count` as our
nonnegative random variable. The 90th percentile thus can't
exceed `min_count + 10 * (expected_iter - min_count)`.  These bounds
are extremely conservative, but might still be useful.  Numerical
experiments can provide stronger bounds, e.g., with a
[binomial test](https://github.com/pkhuong/csm).
