Sequential Kolmogorov-Smirnov tests with power 1
================================================

This is an implementation of Darling and Robbins's Kolmogorov-Smirnov
test described in [nonparametric sequential tests with power one](https://www.pnas.org/content/pnas/61/3/804.full.pdf).

The interface and usage is similar to the [Confidence Sequence
Method](https://github.com/pkhuong/csm).  However, rather than testing
a binomial distribution for its parameter, this library offers a
sequential versions of the
[Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)
test for goodness of fit between arbitrary distributions over ordered
values.

Like the [CSM](https://arxiv.org/abs/1611.01675), the methods can
safely be ran an unbounded number of times as the sample gains more
data points: the false positive (rejection of the null hypothesis when
the two distributions are actually equal) for an unbounded number of
trials is bounded by the `log_eps` parameters, since the method
internally corrects for multiple hypothesis testing.  Each new
observation increases the stringency of the test, so any actual
difference will eventually (with high probability) be detected, if we
run the method for long enough.

*If we let the method run forever*, we thus have a semialgorithm with
one-sided error: while we may spuriously reject the null hypothesis
with probability controlled by the `log_eps` parameter, we will always
eventually correctly reject the null.

The basic one-tailed test
-------------------------

The code directly implements the one-tailed test with two samples.
Given an unbounded stream of observations from distribution A and
distribution B, this one-tailed test determines whether the cumulative
distribution function (CDF) for A is always less than or equal to that
for B.  When the values track response latencies, this is equivalent
to asking whether, for every quantile, A is always slower than B.  If
the test rejects the null hypothesis, this means there is at least one
quantile at which A is faster than B.

There are two knobs over the expected runtime to correctly reject the
null hypothesis: the rate of false positive, and the number of
observations we accumulate before attempting to pass judgement.

Allowing a higher probability of falsely rejecting the null lets us be
more aggressive.  In the extreme, allowing 100% false positive would
let us reject the null before observing a single value (: We can
usually afford large sample sizes when data are generated by running
computer programs, so we should aim for very low false positive rates.
That's why the interface accepts the *natural log of* the allowed
false positive rate, `log_eps`.

The `min_count` is the minimum number of observations we'll accumulate
before testing the hypothesis. With more data, we can be more
confident, so not testing the hypothesis at all when we have too
little data lets us spend our "false positive budget" later, and thus
also be slightly more aggressive.

Given `n` pairs of data point, `one_sided_ks_threshold(n, min_count,
log_eps)` will give us the threshold in maximum CDF difference to
reject the null this iteration.  We can then look at the two empirical
CDFs, and find the value for with the CDF of A is maximally greater
than that of B.  If there is none (the empirical CDF of A is always
lower than that of B), we can't reject the null hypothesis.  Whenever
the difference is greater than the value returned by
`one_sided_ks_threshold`, we can reject the null hypothesis.
Otherwise, we must wait and get more data.

Additional tests
----------------

We sometimes want a two-tailed test, where we simply want to know if
two samples come from different underlying distributions.  That's
equivalent to refuting that the two CDFs are equivalent.  Instead of
comparing the maximum difference between the two CDF against
`one_sided_ks_threshold`, we can simply compare the maximum absolute
difference between the two CDFs against `one_sided_ks_threshold(n,
min_count, log_eps + one_sided_ks_pair_eq)`.  Adding that constant is
equivalent to dividing the allowed false positive rate by 2, i.e.,
we're simultaneously testing whether `CDF A >= CDF B` and vice versa,
with a [Bonferroni
correction](https://en.wikipedia.org/wiki/Bonferroni_correction).

We need a similar correction when testing to refute that an empirical
CDF is less than or equal to a fixed distribution: add
`one_sided_ks_fixed_le` to the original `log_eps`.  When testing for
equality with a fixed distribution, add `one_sided_ks_fixed_eq`.

Finally, we can also test whether points from a single sample come
from any distribution in a family of distributions (e.g., the set of
all univariate Normal distributions).  See [Darling and
Robbins](https://www.pnas.org/content/pnas/61/3/804.full.pdf) for more
details.  In that case, we'll find the instantiation that minimises
the difference between the empirical CDF and the distribution, and
compare that difference with the value returned by
`one_sided_ks_threshold`, with `log_eps = log(false_positive) + one_sided_ks_class`.

Discontinuous distributions
---------------------------

The classic Kolmogorov-Smirnov (e.g., as described by Darling and
Robbins) is defined on continuous distributions, without
discontinuities (e.g., not jump in CDF between 0.99999... and 1).  In
practice, discontinuous distributions are useful and show up a lot.

Thankfully, treating the distance between two discrete (or mixed)
distributions as though they were continuous does not compromise
correctness.  It simply means that we're more likely to accept the
null hypothesis. See [this 2017 paper](http://openaccess.city.ac.uk/18541/)
for more details, examples, and an idea of what the adjustment might
look like.  Our tests all have power 1, so this really means that
ignoring the discontinuity (and associated lower degrees of freedom)
simply means that we'll require more data to reject the null
hypothesis, when it does not actually hold.

More notes on usage
-------------------

If you're not sure whether your compiler is standard compliant, check
that `one_sided_ks_check_constants` returns 0.

Not all pairs of `log_eps` and `min_count` are valid.  Check validity
with `one_sided_ks_min_count_valid`.  When `min_count` is invalid,
that's because it is too low.  `one_sided_ks_find_min_count` will find
the least valid `min_count` value for a given `log_eps`.

Finally, one might want a terminating algorithm rather than a
semialgorithm that also has power one.  For such practically minded
people, there is `one_sided_ks_expected_iter`.  Given a (valid) pair
of parameters `min_count` and `log_eps`, and `delta`, the actual
maximum distance between two underlying CDFs,
`one_sided_ks_expected_iter` will return an upper bound on the
expected number of iterations needed to correctly reject the null
hypothesis.  That upper bound tends to be fairly conservative, but is
nevertheless valid.

We can also convert it into even more conservative bounds on the
median (or other quantile) number of iterations since we know that the
distribution of iteration counts is long-tailed and never goes below
`min_count`.  Simply knowing that the iteration count is a
non-negative value lets us apply
[Markov's inequality](https://en.wikipedia.org/wiki/Markov%27s_inequality):
the 90th percentile can't exceed 10x the expected iteration count.
We can do even better if we also take into account the fact that the
iteration count until we reject the null is always at least
`min_count`, and consider `iteration_count - min_count` as our
nonnegative random variable. The 90th percentile thus can't
exceed `min_count + 10 * (expected_iter - min_count)`.  These bounds
are extremely conservative, but might still be useful.  Numerical
experiments can provide stronger bounds, e.g., with a
[binomial test](https://github.com/pkhuong/csm).

See also
--------

One-sided-KS can only compare full distributions, but does so
relatively efficiently.  The [confidence sequence
method](https://github.com/pkhuong/csm) is even more restricted--it
can only test 0/1 Binomial distributions--but tends to reject the null
hypothesis with fewer data.
[Martingale-CS](https://github.com/pkhuong/martingale-cs) can help
compare more generic continuous point statistics like averages, or
quantiles, but will need a lot more data points.

Can the constants be tighter?
-----------------------------

This test is simply based on a convergent series of false positive
rates, and point-wise comparisons with the Kolmogorov-Smirnov
statistic.  There has been recent (1990-201x) avdancements to really
tighten the constants on the KS statistic; can they help give us
better thresholds?

In the two-sample case, not really.  Darling and Robbins's proposal
hinges on

    P[D+(n) >= r/n] <= exp[-r^2 / (n + 1)]

In [Finite sampling inequalities: an application to two-sample Kolmogorov-Smirnov statistics](https://arxiv.org/pdf/1502.00342.pdf),
Greene and Wellner claim

    P[sqrt(n/2) D+(n) >= t] <= exp[-2 t^2 (2n - 1) / 2n]
                             = exp[-t^2 (2n - 1) / n].

If we let `t = r / sqrt(2n)`, we can reformulate the above as

    P[D+(n) >= r/n] = exp[(-r^2 / 2n) (2n - 1) / n]
                    = exp[(-r^2 / n) (2n - 1) / 2n]
                    = exp[(-r^2 / n) (1 - 1 / 2n)]

Which, while technically better, isn't a big difference (the
multiplicative effect is on the order of `1 + 1 / 2n`)

In the two-sample two-sided case, we could use Fei and Dudley's
[Dvoretzky–Kiefer–Wolfowitz inequalities for the two-sample case](https://arxiv.org/pdf/1107.5356.pdf).

Darling and Robbins simply apply a Bonferroni correction:

    P[D(n) >= r/n] <= 2 exp[-r^2 / (n + 1)]

Fei and Dudley offer, for `n >= 458`,

    P[sqrt(n/2) D(n) >= M] <= 2 exp(-2 M^2)

We can let `M = r / sqrt(2n)`, and find

    P[D(n) >= r/n] <= 2 exp[-r^2 / n]

Again, slightly better than `exp[r^2 / (n + 1)]`, but only for `n >=
458`,  when the difference is negligible.

This `1/n` versus `1/(n + 1)` translates to a `1/n` difference in the
threshold.  Given the scale factor of `sqrt(n)`, this is a difference
of `sqrt(1/n)` data points.  In practice, I doubt this additional
tightness can save more than one iteration of data generation.

The one-sample case is more interesting, mostly because Darling and
Robbins used a loose bound.

Massart found [The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality](https://projecteuclid.org/euclid.aop/1176990746),
in 1990.

In the one-sample one-sided case, we have

    P[D+(n) > \lambda / sqrt(n)] <= exp[-2 \lambda^2],

as long as the right-hand side is at most `1/2`.  We're not that
interested in high false positive rates, so we can satisfy that
constraint by clamping the allowed false positive rate below `0.5`.

If we let `\lambda = r / sqrt(n)`, we find

    P[D+(n) >= r / n] <= exp[-2 r^2 / n]

which is *a lot* better than the bound used for the two-sample
confidence sequence, never mind the one-sample adjustment.

We can recover a more familiar functional form for the error term on
the right-hand side by testing

    P[D+(n) >= sqrt(1/2) r / n] <= exp[-2/2 r^2 / n]
                                 = exp[-r^2 / n]
                                 < exp[-r^2 / (n + 1)]

Again, the two-sided case is a straight Bonferroni correction both for
Darling and Robbins's confidence sequence and in Massart's tight
constant.

While tighter than the two-sample case, this `sqrt(1/2)` factor does
not suffice to beat the direct two-sample bounds: we'd have to sum the
distance between each empirical CDF and the underlying distribution
each, so the `sqrt(1/2)` win would end up as a `sqrt(2)` loss in the
threshold's width.
